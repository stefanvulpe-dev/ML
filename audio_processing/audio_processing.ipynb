{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban sounds classification\n",
    "\n",
    "This notebook is a simple example of how to use the `fastai` library to classify urban sounds. The dataset used is the [UrbanSound8K](https://urbansounddataset.weebly.com/urbansound8k.html) which contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren, and street_music.\n",
    "\n",
    "We will start by importing the necessary libraries and loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to enable the GPU in the settings to speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the dataset class. Besides the `__len__` and `__getitem__` methods, this dataset class makes sure that all samples are loaded with the same duration, resamples the audio if necessary, brings the audio to only one channel, and pads it if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, annotations_file: str, audio_dir: str, target_sample_rate: int, transformation: torchaudio.transforms, num_samples: int, device) -> None:\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[Tensor, str]:\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotations)\n",
    "    \n",
    "\n",
    "    def _get_audio_sample_path(self, index: int) -> str:\n",
    "        fold = f'fold{self.annotations.iloc[index, 5]}'\n",
    "        file_name = self.annotations.iloc[index, 0]\n",
    "        path = os.path.join(self.audio_dir, fold, file_name)\n",
    "        return path\n",
    "    \n",
    "        \n",
    "    def _get_audio_sample_label(self, index: int) -> int:\n",
    "        return self.annotations.iloc[index, 6]\n",
    "    \n",
    "\n",
    "    def _resample_if_necessary(self, signal: Tensor, sr: int) -> tuple[Tensor, int]:\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal  \n",
    "        \n",
    "\n",
    "    def _mix_down_if_necessary(self, signal: Tensor) -> Tensor:\n",
    "        if signal.size()[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "\n",
    "    def _cut_if_necessary(self, signal: Tensor):\n",
    "        # signal -> Tensor -> (num_channels, num_samples)\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[: , :self.num_samples]\n",
    "        return signal\n",
    "    \n",
    "\n",
    "    def _right_pad_if_necessary(self, signal: Tensor):\n",
    "        length = signal.shape[1]\n",
    "        if length < self.num_samples:\n",
    "            missing_samples = self.num_samples - length\n",
    "            last_dimension_padding = (0, missing_samples) # (1, 2) -> (left, right)\n",
    "            # [1, 1, 1] -> [0, 1, 1, 1, 0 , 0]\n",
    "            signal = torch.nn.functional.pad(signal, last_dimension_padding)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample rate used is 22050 Hz, and the duration is 4 seconds. The audio is resampled to 22050 Hz if it is not already in this sample rate. If the audio is stereo, it is converted to mono. If the audio is shorter than 4 seconds, it is padded with zeros. If the audio is longer than 4 seconds, it is truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "NUM_SAMPLES = 22050\n",
    "ANNOTATIONS_FILE = './data/UrbanSound8K.csv'\n",
    "AUDIO_DIR = os.path.join(os.path.curdir, 'data')\n",
    "\n",
    "class_mapping = {\n",
    "    0: 'air_conditioner',\n",
    "    1: 'car_horn',\n",
    "    2: 'children_playing',\n",
    "    3: 'dog_bark',\n",
    "    4: 'drilling',\n",
    "    5: 'enginge_idling',\n",
    "    6: 'gun_shot',\n",
    "    7: 'jackhammer',\n",
    "    8: 'siren',\n",
    "    9: 'street_music'\n",
    "}\n",
    "\n",
    "mel_spectogram = torchaudio.transforms.MelSpectrogram(\n",
    "    SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=64\n",
    ")\n",
    "\n",
    "usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, SAMPLE_RATE, mel_spectogram, NUM_SAMPLES, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for audio classification, we will use a simple neural network with four convolutional layers and one fully connected layers. The model is defined in the `AudioCNN` class. The `forward` method defines the forward pass of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 5 * 4, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, input_data: Tensor) -> Tensor:\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 66, 46]             160\n",
      "              ReLU-2           [-1, 16, 66, 46]               0\n",
      "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
      "            Conv2d-4           [-1, 32, 35, 25]           4,640\n",
      "              ReLU-5           [-1, 32, 35, 25]               0\n",
      "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
      "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
      "              ReLU-8           [-1, 64, 19, 14]               0\n",
      "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
      "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
      "             ReLU-11           [-1, 128, 11, 9]               0\n",
      "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
      "          Flatten-13                 [-1, 2560]               0\n",
      "           Linear-14                   [-1, 10]          25,610\n",
      "          Softmax-15                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 122,762\n",
      "Trainable params: 122,762\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.83\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 2.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = AudioCNN().to(device)\n",
    "summary(model, (1, 64, 44))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: nn.Module, optimizer: torch.optim.Optimizer, num_epochs: int, device: str) -> None:\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch} - Loss: {loss.item()}')\n",
    "\n",
    "def predict(model: nn.Module, input: Tensor, target: int, class_maping: dict) -> Tensor:\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "        _, predicted_index = torch.max(output, 1)\n",
    "        predicted_class = class_maping[predicted_index.item()]\n",
    "        expected_class = class_maping[target]\n",
    "        predictions.append((expected_class, predicted_class))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 2.2592251300811768\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = DataLoader(usd, batch_size=128, shuffle=True)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train(model, train_data_loader, loss_fn, optimizer, 10, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: dog_bark, Predicted: dog_bark\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(model, usd[0][0].unsqueeze(0), usd[0][1], class_mapping)\n",
    "print(f'Expected: {predictions[0][0]}, Predicted: {predictions[0][1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
